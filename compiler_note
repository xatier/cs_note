2/25
====

The dragon book!

super computing: cooling!

purple dragon: the most evil dragon

project: 45%   # compiler implementation: C--
midterm 25%
final 25%
quiz: 5%

we have 6 check point for the project

automatic translation & optimization

the traditional view is a narrow view

translation!

silicon compiler: Ex. verilog to VLSI layout

台灣芯悲劇 XD

target machine mode
    pure machine code (ISA)
        assembly language format, relocatable binary format, load-and-go-format
    augmented machine code (ABI)
    bytecode(virtual machine code)

P-code (Pascal)
[wiki](http://en.wikipedia.org/wiki/P-code_machine)

VISA -> virtual ISA


two parts of compilation
    analysis: tokenise, AST, ...
    synthesis: codegen, optimize ...

the difference between interpreter and compiler


interpreter avoids the synthesis part but involves large overhead(time, space)
    portable, dynamic typing
    better diagnostics

hybrid compiler / JIT

a head of time / JIT (adaptive translation) -> profiling


lexical analysis (linear analysis)
syntax analysis (hierarchical analysis)
semantic analysis

codegen
opt



syntax directed compiler

lexical analysis:  error if unrecognized token
syntax analysis: error if can't build ASTs
semantic analysis: type checking! insert type conversion to AST

phases of a compiler

six phases: lexical, syntax, semantic, IR codegen, optimizer, target codegen
symbol table manager / error handler and recovery

optimization: machine dependent optimization / machine independent optimization

setroation elesmatic



3/4
====

review the 6 phases

there's no real registeer in IR, use virtual registeer instead

lexical analyis: FSA (Finite State Automata)
codegen: Tree matching

frondend / backend

common backend compiling system
    variety languages -> IR -> optimizer -> target machine codegen

retargetable compiler
    one language front-end -> IR -> machine independent optimization -> different machine codegen

without IR -> N by M

syntax (structure)
semantics (meaning)

syntax is usually defined by CFG (context free grammar)
semantics: such as type compatibility, scoping rules, context-sensitive

static semantics
    attribute grammars
runtime semantics
    natural semantics
    axiomatic semantics
    denotational semantics

    Ex. assertion

imprecise semantics challenges
    sometimes definitions in the specification of the language are not well defined
    unreachable statements (control may reach end of non-void function)



requirements for a successful languages
many machines failed because they are hard to compile/optimize for
    intel 860    IBM cell    cray-2

codegen
parsing
code optimization
parallelization

instruction parallelism!

cross compilation



ch 2.
design of a simple compiler

type check!

AC program char stream -> lexical analyzer -> token stream -> syntax directed translator -> DC code

AC-to-DC compiler



3/11
====

my laptop ran out of power QQ

tokenization
parsing

parsing tree
    root: start sumbol
    leaf: token or empty
    internal node: non-terminal

ambiguous

grammar: can be recursive, but notice if any ambiguity
    associative operators


associativity

<expr> -> <primary> { <op> <primary> }     Ex. A = {+ B} {+ C}
<expr> -> { <primary> <op> } <primary>     Ex. {A = } { B =} C

<expr> -> <expr> <op> <primary>  : left
<expr> -> <primary> <op> <expr>  : right

focus on what is repeating
    op primary -> left associativ
    primary op -> right associative


precedence of operators
    create two non-terminals for the two levels of precedence
    practice: give a weight (priority) to operators, using a stack to maintain that
    operator precedence parsing

parse tree / syntax tree

top down
bottom up

top down: known as predictive parsing, may need to modify productions
bottom up: less restrictions on grammars, more commonly used in production compilers

recursive descent parsing

for each non-terminal has a parsing procedure
for symbols on RHS, to match a non-terminal A, call the parsing procedure A, to match a terminal t, all math(t)

lookahead
  ll(K): need K lookaheads to determine the path (uniquely)
  the next_token is used to determine what production to apply
  first(alpha): grab the first token out

not all CGF have the property of predictive parsing

productions sharing the same LHS can be distinguished by the FIRST() set.
with the above property is called  LL(1) grammar
LL(1) stands for left-to-right, left-most derivation, only one lookahead token is required in predictive parsing


try LL(1) as possible otherwise, LR parsing

left recursion removal

A -> Aa | b

{b, ba, baa, baaa, ...}

A -> bR
R -> aR | e

(e for empty)


we need to make left recursion "right"

LL -> top down
LR -> bottom up

left recursion & left factoring

left recursion is good to bottom up parsing, but bad to predictive parsing







3/18
=====

AST after semeantic analysis

in symbol table, we always use a union to represent different types

type chacking

symbol reference

using visitor pattern

codegen in AC-DC compiler

temporaries
in real compilers, registers are typically used as temporaries


register is not memory

locality / parallelism

move computation to data or move data to computation

top down parsing: build the parse tree top down left to right
recursive descent: one way to implement top-down parsing, may need to backtrack
predictive parsing: recursive descent parsing when the CFG is LL(k), no backtracking

semantic routines
semantic records

syntax directed definition: CFG + semantics routines
(semantic attributes + semantic rules)


source program <-char stream (maybe push back)-> lexical analyzer <-pass token, attribute value, get next-> parser





3/25
=====

a fixed decimal literal
\d?\.\d?  # wrong
\d?\.\d+ | \d+\.\d?  # correct

note '.' is illegal

beware, optional type usually can't be 'all clear'

A?B?            -> matches an empty string
AB? | A?B       -> A or B must be one or more


none regular set

    Ex. no balanced / nested structure / recursion

FA
    finite automata and scanners

RE -> FA


NFA -> DFA
    - no e-transition
    - unique successor states


subset construction algorithm

compiler directives

lexical error & recovery

overlapped regular expression

beware RE is greedy (longest match)


transition table -> sparse matrix

    table compression: eliminating redundant rows, pair compressed transition tables


grouping state

from RE to NFS - 3 basic operations (by Ken Thompson)



4/1
====


April fools day

scanner generator -> parser generator

why using CFG?
    precise syntactic specification of a programming language
    automatic efficient parser generator
    enabling automatic translator generator
    language extension becomes easier


the role of the parser
    taking tokens from scanner, parser, reporting, syntax erros
    not just parsing, in a syntax-directed translator, the parser also conducts type checking, semantic analysis and IR codegen
    -> the new style: multi-pass compiler (apply several passes on AST)


Tn: non-terminal
Tt: terminal



if A -> \gamma, then \alphaA\Beta => \alpha\gamma\Beta

one or more steps derivation: =>+
zero or more steps derivation: =>*


sentential form: some terminal and non-terminal

SF(G): is the set of sentential form of grammar G

what is a sentence of language L defined by the C++ grammar G?
    a C++ program


can 'z' be an empty string?

terminal string: input program
   can the input program accepted by the grammar?


left-most derivation: top-down parsing is to come up with a left-most derivation

right-most derivation: bottom-up parsing is to discover a right most derivation, it reverses the derivation

A phrase sequence of symbols descended form a single non-terminal in the parse tree
A simple or prime phrase is a phrase that contains no smaller phrase


the handle of a sentential form is the left-most simple phrase

if there's a handle: it's time to reduction!

point: make simple phrase to non-terminal


useless nonterminals
reduced grammar
    a grammar is reduced after all useless non-terminals are removes
ambiguity


[dangling else](http://en.wikipedia.org/wiki/Dangling_else)

parser and recognizer

recognizer
   an algorithm that does boolean-valued test
   is this input syntactically valid?

parser
answers more general questions
is the input valid
and, if it is, what is its structure (parse tree)

handle found! -> reduce!

actually, we use LR(0.5) (? in practice

because LR(1) is too powerful but generates too many states

in general, LL(1) or LR(1) (lookahead = 1) is enough

grammar analysis algorithms
    iterative marking algorithm

Follow(A)
First(\alpha)


Follow(A) is the set of terminals that may follow A in some sentential form.
It provides the lookaheads that might signal the recognition of a production with A as the left hand side
(scan the grammar)

First(\alpha)
The set of all the terminal symbols that can begin a sentential form derivable from \alpha

If \alpha is the right-hand side of a production,
then First(\alpha) contains terminal symbols that begin strings derivable from \alpha

you don't understand what is parsing lol



remember the $ sign (EOF) in follow

computation of predict set



4/8
====

LL(1) action symbols

sementic stack != parse stack
semantic stack is a stack of semantic records: attribute info
actoin symbols are pushed to the parse stack

LL(1) conflict
  we do not know which production to use when we have multiple choose in one predict set

  - common prefiex -> left factoring
  - left recursion

First(P): maybe contains lambda
Predict(A->P): if there's a lambda, we need to plus Follow(A)


PRedict(A->X1 ... Xm) = 
    if \ in First(X1 ... Xm)
        (First(X1 ... Xm) - \) U Follow(A)
    else
        First(X1 ... Xm)

First(A) different from First(A->P)
    maybe A contains multiple production


LR parsing

LR parsing is attractive
can recognize virtually all programming language constructs
the most general non-backtracking parsing method known
more powerful
efficient LR parser generators available

4 actions: shift reduct error accept

each state summarizes the info contains in the stack below it

parsing table
    states: the key to recognize a handle
    goto table
    action table


items can ve viewed as the states of an NFA

-> apply 'subset construction algorithm' to a NFA
    "grouping" -> configuration set
    -> items that may be applicable at a given point of parse

    represents a situation in which an id can be matched as part of any of three productions

    closure: states that can be reached (here, * for the dot)
        if A -> \alpha * B \beta is in closure(l) and B -> \gamma is a production
        then add the item B -> * \gamma to l
        repeat until no more item can be added

        note: if we have a left recursion, don't add!

CFSM: characteristic FSM / collection of set of LR(0) items

LR(0) parse table




4/22
=====

midterm review

LR(1)

put lookaheads to reduction rules


powerful
LR(0) -> SLR(0) -> LALR(1) -> LR(1)

but LR(1) usually brings much larger table size, so we use LALR(1)


semantic actions and values
synthesized and inherited attributes

each production can have an associatted code sequence that will execute when the production is applied


parse stack / value stack

parse stack is responsible for syntax analysis
value stack is responsible for semantic analysis

single pass approach would be difficult to understand, extend and maintain
multi-pass compilation approach is to define an IR. each of the semantic analysis, program optimization, and codegen work, will be treated in separate passes. AST is such an IR


how to build a AST


4/29
====

symbol table

one big table (faster search)
multiple small table (slower search)

how about C++ ADL?

multiple small table is good for keeping symbol tables of closed scopes

name / type / var / level / hash / depth

unordered list
ordered list: bin search
binary search tree
hash tables
    constant time search
    most commonly used
    conflicts resolution: chaning is commonly used


name space

overloading

function pointer (nested functions)
    gathering closure info and produce related code during codegen


forward references



5/6
====

type checking

static type checking
dynamic type checking

type checking of expressions
    work with AST

type conversion

implicit: coercions
explicit: casting

structurally equivalent
name equivalent


modern languages tends to use name equivalence rules
type comparison is just a comparison of pointers
programmers can get full benefit of type checking
useful for type-inferred aliasing analysis less possible aliasing
it's hard to implement structural equivalence


type inferred aliasing analysis

encoding the type expression

value numbering

common subexpression elimination


semantic analysis


Java exceptions rules

    catch or declare rule

