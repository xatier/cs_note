2/17
=====

automation of intelligent behavior


homework: 15%
quiz: 15%
tests: 40%
projects: 30%

(tesxtbook)[http://aima.cs.berkeley.edu/]


2/20
====

history of AI

given a problem and some data, make an algorithm to develop some solution

[Sebastian Thrun: Google's driverless car](http://www.ted.com/talks/sebastian_thrun_google_s_driverless_car.html)


standard <-> sensors <-> feedback

- [learning agents](http://en.wikipedia.org/wiki/Intelligent_agent)

feedback make the system changes
actuators is the part `act`s the model


AI: MIT, Stanford, CMU

article analysis system

given a problem, we build a model to analyze that, and then prediction?

start: modeling the system

classify the sample to different models

modeling human brain: neural network

    different from a `computer`

evolutionary computing

[Genetic algorithms](http://en.wikipedia.org/wiki/Genetic_algorithm)

represent the model as a set of gene


feed-forward network: multi-layer structure of our brains

natural language

learning from examples (samples)

1. missionary problem
1. p1 p2 p3


2/24
====

the study of human emotion is the golden goad

common sense reasoning

knowledge representation + operation (or constraint)

coloring problem

good knowledge representation makes the problem easier


2/27
====

machine learning

brain is a massively parallel information processor of highly interconnected

synapses(weights)

activation function
sigmoid function: 1/(1+e^-x)

g' = g(1-g) => a normal distribution, the maximum point is 1/4

take neural units as logic gates

input units -> hidden units -> output units

recurrent network: feedback system

perceptron: a single layer feed forward neural network

linear separator / hyperplane

error surface

perceptron learning

reduce the error, update the weights (ignore tiny strength signals)




3/6
====

examples
  |
  | decision tree algorithms
  V
decision tree (concept)
  |
  |
  V
classification



** homogeneous sets
** non-homogeneous set (can't tell things obviously)

if the test is non-homogeneous, maybe we need more tests to make that homogeneous

the idea is adding tests to make it homogeneous

tests generally has a cost, so we fewer nodes tree is better

entropy

information gain -> make features more obvious

overfitting issues

cross validation




nearest neighbor learning
instance based learning
voronoi diagram



3/10
====

feature extraction


consider the angle (as distance between neighbors)

Error function

give weight for each dimension

but the solution may get a local optimized solution





3/13
====

probability model

EM algorithm


3/17
====

genetic algorithm

TSP

中國郵便士問題

mimic natural selection

1. create an initial population of one chromosome
2. mutate one or more genes, producing one new offspring for each chromosome mutated
3. mate one or more pairs of chromosome
4. add the mutated and offspring chromosome to the current population
5. select a best generation and randomly pick up 2nd, 3rd

genetic programming



3/20
=======
3/27 midterm
4/3 holiday


penalize the example of misclassification

positive: e/1-e, or sqrt( e/1-e)
negative: 1-e/e

Cauchy–Schwarz inequality



3/31
====

Google deepmind


4/10
====

zzz

4/24
====

A* and heuristic

consistent heuristics

n-queens problem
