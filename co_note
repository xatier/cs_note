9/16
=====

https://sites.google.com/site/nctuco/

grading policy
    midterm 20% *2
    final 20%
    homework/programs 40%

problem solving ability
critical thinking
interpersonal communication skills
group based interaction skills


you must document your work

Laptops are discouraged ... Orz

Grasp info from the web

Moors's Law
    The number of transistors per square-inch doubles each 18 months

logic capacity
clock rate

memory
disk


Google's solution

More Moore: Miniaturization
More than Moore: Diversification

ISCA

the history
http://isca2013.eew.technion.ac.il/


9/23
======

performance

measure, report, and summarize
response time (latency per job)
throuphput (system performance)
    單位時間能跑的 job 個數

Elapsed time
    counts everything (useful number, but oftern not good for comparison purposes)
CPU time
    doesn't count I/O and other programs
    sustem time / user time
our focus: user CPU time
    time spent executing in our program
Performance
    performance = 1 / execution time


cycle time
Law of Performance
CPI (cycle per instruction)
MIPS
MFLOPS
SPECCmarks
Benchmarks
Averageing

CPU clocking
operation of digital hardware governed by a constant-rate clock

clock frequency
clock period

cputime = time / program
        = (instruction/program) * (cycles/instruction) * (time/cycle)


cycletime: time/cycle
instruction per program
    ISA and compiler
CPI
    ISA and architecture
time per cycletime -> time between ticks
    organization, hardware, technology, layout

[intel tick-tock model](http://en.wikipedia.org/wiki/Intel_Tick-Tock)


performance depends on
    algorithm: IC (instruction count), possibly CPI
    programming language: IC, CPI
    compiler: IC, CPI
    ISA: IC, CPI, Tc


MIPS million instruction per second




9/26
========

MIPS

(instruction / program) * (program / time) x 10^-6

= (inst count / (exec time * 10^-6))
= clock rate / CPI * 10^-6

meaningless indicator of performance


Ex. two compilers
cpu cycles
Class A: 1
Class B: 2
Class C: 3

compiler 1: 5M class A, 1M class B, 1M class C
compiler 2: 10M class A, 1M class B, 1M class C

CPI1 = (5*1 + 1*2 + 1 * 3)/(5+1+1)

MFlops: mega flops (floating point operation)


Benchmarks
Performance best determined by running a real application
small benchmarks
spec (system performance evaluation cooperative)

arithmetic mean
    means for times, CPI
harmonic mean
    means for rates, MIPS, MFLOPS
geometric mean
    normalize numbers


算幾不等式...

(a+b)/2  >   sqrt(ab)   >    2/(1/a + 1/b)

算數平均會比較樂觀

SPARC10


Amdahl's Law

execution time after improvement =
execution time unaffected + (execution time affected / amount of improvement)

Principle: Make the common case fast

improvement to be gained from using faster mode is limited buy the fraction of the time the faster mode can be used


power if the first class design issue

manufacturing IC
Yield: proportion of working dies per wafer



----

ALU

number representation

two's complement

add/sub

A - B = A + (-B) = A + B' + 1

and/or/add/sub/slt





9/30
=====
slt
set on less then

result = 1 when (a < b)
the last bit + a lot of 0

actually, it's a subtraction

EQ: a-b = 0, implies a = b


improve adder by carry lookahead
ripple carry adder is slow

when generate carry? gi = ai & bi
when propagate the carry? pi = ai | bi



10/3
=======

zzzZZZZ

multiplication

Booths

IEEE floating point format
    S-Exponent-Fraction
       8/11    23/52
x = (-1)^s * (1+Fraction) * 2^(exponent -Bias)

sign bit: 0-> non negative, 1-> negative
normalization significand: 1.0 <= |significand| < 2.0

exponent : actual exponent + Bias
ensures exonent is unsigned
Bias: 127/1023

floating point addition
align decimal points
add significands
normalize result & check for over/underflow



instruction set

instruction format or encoding
    fixed, variable, or hydrid




10/7
==========

classes of instruction operations

data movement instruction
arithmetic and logic
branch instruction


memory 100
L2 10
L1

ftp id/password?
    CO2013
    87654321


classifying isa by location of operations
stack
accumulator
memory-memory
register-memory
load/store machine
    memory operand only appears in load/store instructions




endian convention
endian
aligned, misaligned

ISA design principles


MIPS
RISC
32-bit byte addresses
load/store onlydisplacement
registers

3 operands machine
    op dst, src1, src2

each register contains 32-bits

10/17 computer organization
===========================

constant: immediate

larger constant: use more instructions

lui: load upper immediate

in ARM case, 

    mov r0, 0x68000000
    mov r0, 0x68, (LSL #24)

take a immediate but shift some zeroes to it




procedure calling
-----------------

    arguments
    result values
    temporaries
    saved    # must be saved by callee

    $gp: global pointer (reg 28)
    $sp: stack pointer (reg 29)
    $fp: frame pointer (reg 30)
    $ra: return address (reg 31)


there's no `mov` in MIPS...
use `add 0` instead

try to map each arguments in registers, reduce the usage of stack (load/store)


arguments in `$a0, $a1 ...`
return value in `$r0`

local data allocated by callee
procedure frame (activation record)



.data
.code
.symbol




----

ARM instructions
----------------


most popular 32-bit instruction set in the world
embedded core market

typical of many modern RISC ISAs

16x32-bit register file

32-bit data called "word"
register: r1-r15


Data processing instructions

opcode
Rd destination
Rn source
Operand2
immediate
set
condition -> reduce the usage of branches and jumps


pipelining

most constants are small

nop cost in ARM



----


x86 instruction
---------------

blah


----

circuit
-------

edge-triggered clocking

Latches and Flip-Flips
- change of state
- latches
- flip-flop


components for datapath

- instruction fetch
- R-type instructions
- memory and sign extension




the interface of R-type instructions

大家有沒有注意到，這邊有個 mux ，他的意思就是選擇器


10/21 computer organization
===========================

alu source

try to trace the ALU via simple instructions

memory 2 register mux

register destination mux


register file

register write enable

the opcode contains these enable/selection signals

if we don't need L2 cache, just disable it



beq instruction

PC = PC + 4 is the default behavior

but PC will be erased by branch instruction


10/24 computer organization
=========

load code/target addressing

# !!!
# 10/31 CO midterm
# !!!

R-type / load / branch instructions

implementing Jump


performance of single cycle machines

    - add up the path instruction passed

    - Cycle time -> try to find the worst case in the circuit




10/28
========

pipelining

parallelism improves performance

5-stages pipelining: IF ID EX MEM WB

add some flip-flops between stages

Harvard
    structual / control / data

pipelining data path



11/4
=====

data path control signals



11/7
=====
design of pipelining
all instructions are the same length
just a few instruction formats, symmetry register fields


control hazards

one solution is insert some `nop`s to fit the pipeline width

    - not so good (?)

another solution is register renaming
    http://en.wikipedia.org/wiki/Register_renaming



11/11
=======

hazard maybe cause by data dependency

hardware stall

forwarding

the implementation of forwarding


11/18
=======

branch prediction


11/21
=====

branch hazards

stall
recding branch delay b early determination of branch
instruction schedling for branch hazard
branch condition prediction
    1-b buffer
    2-b buffer

moving branch decisions earlier in pipe?

flush the misprediction

delayed branch
    delay slot

from before branch
from branch target
from fall through

branch target buffer



11/28
======

design for i7 CPU

L2 cache -> for core itself
L3 cache -> sharing to 4 cores

power efficiency

predicated instruction: like arm
    pipeline related ISA design


http://www.arm.com/products/processors/technologies/biglittleprocessing.php

delayed branch
    execute instructions followed by branch instructions, then do jumping
    advanced pipeline have long slots

memory gap

memory hierarchy

cache line is a unit for memory fetching

hits / miss

   write-through / dirty





12/2
======


associative search

direct mapping

block address / block offset
tag / index / offset

read data from index

give a cache line size, ask address

4way
8way associative (full associative)



12/5
======

software solution
   delayed branch
   delay slots

hardware solution
    hazard detection

L1 shouldn't <= 4way

LRU (least recently used)

locality: might accessed again soon
    spatial locality (linear neighbors)
    temporal locality (the same object)

write through or write back?


write through
    write to the memory directly
write back
    only mark the dirty bit

write buffer between cache and memory

16words/block = 64Bytes/block

Example: Intrinsity FastMATH

AMAT
average memory access time

hit time + miss rate x miss penalty

in small cache cases, block size is important





12/16
=====


decreasing miss penalty with multilevel caches

L1: unify
L2: split

increasing memory bandwidth

capacity
conflict
compulsory



